{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOnMGtuTLX4za3AmXdYDXpH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"LGl5jnhwqU7S"},"outputs":[],"source":["\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Define vocabulary size and embedding dimensions\n","vocab_size = 6\n","embedding_dim = 6\n","num_heads = 2\n","\n","# Initialize word embeddings\n","torch.manual_seed(42)\n","embedding_layer = nn.Embedding(vocab_size, embedding_dim)\n","word_indices = torch.arange(vocab_size)\n","word_embeddings = embedding_layer(word_indices)\n","\n","# Define Multi-Head Self-Attention Class\n","class MultiHeadSelfAttention(nn.Module):\n","    def __init__(self, embed_dim, num_heads):\n","        super().__init__()\n","\n","        if embed_dim % num_heads != 0:\n","            raise ValueError(\"Embedding dimension must be divisible by the number of heads\")\n","\n","        self.num_heads = num_heads\n","        self.head_dim = embed_dim // num_heads\n","\n","        self.W_Q = nn.Linear(embed_dim, embed_dim, bias=False)\n","        self.W_K = nn.Linear(embed_dim, embed_dim, bias=False)\n","        self.W_V = nn.Linear(embed_dim, embed_dim, bias=False)\n","        self.W_out = nn.Linear(embed_dim, embed_dim)\n","\n","    def forward(self, x):\n","        batch_size, seq_length, embed_dim = x.shape\n","\n","        Q = self.W_Q(x)\n","        K = self.W_K(x)\n","        V = self.W_V(x)\n","\n","        Q = Q.view(seq_length, self.num_heads, self.head_dim).transpose(0, 1)\n","        K = K.view(seq_length, self.num_heads, self.head_dim).transpose(0, 1)\n","        V = V.view(seq_length, self.num_heads, self.head_dim).transpose(0, 1)\n","\n","        d_k = self.head_dim ** 0.5\n","        attention_scores = (Q @ K.transpose(-2, -1)) / d_k\n","        attention_weights = F.softmax(attention_scores, dim=-1)\n","\n","        attention_output = attention_weights @ V\n","        attention_output = attention_output.transpose(0, 1).contiguous().view(seq_length, embed_dim)\n","\n","        output = self.W_out(attention_output)\n","        return output, attention_weights\n","\n","# Instantiate the attention layer\n","self_attention = MultiHeadSelfAttention(embedding_dim, num_heads)\n","\n","# Prepare embeddings for processing\n","word_embeddings = word_embeddings.unsqueeze(0)\n","output, attention_weights = self_attention(word_embeddings)\n","\n","# Print results\n","print(\"Attention Weights:\\n\", attention_weights)\n","print(\"\\nFinal Output After Attention:\\n\", output)"]}]}